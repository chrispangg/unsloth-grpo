{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this script to load from checkpoint and save the checkpoint to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/grpo_demo/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 02-17 21:39:56 __init__.py:190] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.5: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.448 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.5 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = (\n",
    "    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    ")\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "checkpoint = \"chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF/checkpoint-17500\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=checkpoint,  # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"If a box of apples costs $15 and contains 30 apples, what is the cost of each apple? Also, if I want to buy 7 apples, how much would I need to pay?\",\n",
    "    },\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(\n",
    "    input_ids=inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "==((====))==  Unsloth 2025.2.5: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.448 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.5: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.448 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "Test 1/1\n",
      "Prompt: Janet works part-time at a grocery store. She works 4 hours a day, 3 days a week, and earns $12 per hour. How much does she earn in a month (4 weeks)?\n",
      "\n",
      "Base Model Response (20.52s):\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "user\n",
      "\n",
      "Janet works part-time at a grocery store. She works 4 hours a day, 3 days a week, and earns $12 per hour. How much does she earn in a month (4 weeks)?assistant\n",
      "\n",
      "To find out how much Janet earns in a month, I need to calculate her total earnings for 4 weeks. Let's break down the problem step by step.\n",
      "\n",
      "First, let's figure out how many hours she works in a month. Since she works 4 hours a day, 3 days a week, and there are 4 weeks in a month, I can multiply the number of hours per day by the number of days per week and then by the number of weeks.\n",
      "\n",
      "Calculating hours per week: 4 hours/day * 3 days/week = 12 hours/week.\n",
      "\n",
      "Then, multiply that by the number of weeks in the month: 12 hours/week * 4 weeks/month = 48 hours/month.\n",
      "\n",
      "Now, Janet's hourly wage is $12. So, her monthly earnings would be total hours multiplied by hourly wage.\n",
      "\n",
      "Total earnings = 48 hours/month * $12/hour = $576/month.\n",
      "\n",
      "Wait, let me double-check that. If she works 4 hours a day, 3 days a week, that's 4*3=12 hours per week. Then 12 hours/week *4 weeks/month=48 hours/month. Yes, that seems right. So 48 hours times $12 per hour is $576. That checks out.\n",
      "\n",
      "Alternatively, another way to calculate is to convert the hours to days and then multiply by the weekly wage. But since the problem gives hours per day, that's straightforward. So 4 hours/day * 3 days/week * 4 weeks/month = 48 hours/month. Then 48 *12 = 576. Yep, same result.\n",
      "\n",
      "I think that's correct. Let me just make sure there's no trick in the question. It says she works 4 hours a day, 3 days a week. So each day she works 4 hours, each week 3 days, and each month 4 weeks. So the calculation is straightforward. No hidden assumptions here. The answer is $576 per month.\n",
      "\n",
      "**Final Answer**\n",
      "\\boxed{576}\n",
      "\n",
      "assistant\n",
      "\n",
      "assistant\n",
      "\n",
      "<|begin_of_thought|>\n",
      "\n",
      "Okay, let's see. Janet works part-time at a grocery store. She works 4 hours a day, 3 days a week, and earns $12 per hour. I need to find out how much she earns in a month, which is 4 weeks. Hmm, let's break this down step by step.\n",
      "\n",
      "First, I need to figure out how many hours she works in a month. Since she works 4 hours a day and 3 days a week, I should multiply those two numbers together. That makes sense because if you have more hours in a day and more days in a week, you get more hours in a month. So 4 hours/day * 3 days/week = 12 hours/week. Then, multiply that by the number of weeks in a month. There are 4 weeks in a month, so 12 hours/week * 4 weeks/month = 48 hours/month. That seems right.\n",
      "\n",
      "Now, she earns $12 per hour. So, to find her monthly earnings, I multiply the total hours by her hourly wage. That would be 48 hours * $12/hour. Let me do that calculation. 48 * 12. Hmm, 40 * 12 is 480, and 8 * 12 is 96, so 480 + 96 = 576. So that's $576 per month.\n",
      "\n",
      "Wait, let me check again to make sure I didn't make a mistake. If she works 4 hours a day, 3 days a week, that's 4*3=12 hours per week. Then 12 hours/week *4 weeks/month=48 hours/month. Yes, that's correct. So 48 hours *12 is indeed 576. I think that's right.\n",
      "\n",
      "Alternatively, another way to think about it is to convert the hours to days and then multiply by the weekly wage. So 4 hours/day *3 days/week *4 weeks/month=48 hours/month. Then 48*12=576. Same answer. So that's another way to confirm it.\n",
      "\n",
      "I don't see any mistakes in the reasoning here. The key steps are calculating the total hours per month, then multiplying by the hourly wage. All the numbers check out. So the final answer should be $576 per month.\n",
      "\n",
      "<|end_of_thought|>\n",
      "\n",
      "<|begin_of_solution|>\n",
      "\n",
      "To determine Janet's monthly earnings:\n",
      "\n",
      "1. **Calculate total hours per month**:  \n",
      "   - Hours per day: \\(4 \\, \\text{hours/day}\\)  \n",
      "   - Hours per week: \\(4 \\, \\text{hours/day} \\times 3 \\, \\text{days/week} = 12 \\, \\text{hours/week}\\)  \n",
      "   - Hours per month: \\(12 \\, \\text{hours/week} \\times 4 \\, \\text{weeks/month} = 48 \\, \\text{hours/month}\\).\n",
      "\n",
      "2. **Multiply by hourly wage**:  \n",
      "   - Hourly wage: $12 \\, \\text{per hour}  \n",
      "   - Total earnings: \\(48 \\, \\text{hours/month} \\times 12 \\, \\text{per hour} = 576 \\, \\text{dollars/month}\\).\n",
      "\n",
      "**Answer**: Janet earns \\(\\boxed{576}\\) dollars per month.\n",
      "\n",
      "<|end_of_solution|>\n",
      "\n",
      "Finetuned Model Response (24.04s):\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "user\n",
      "\n",
      "Janet works part-time at a grocery store. She works 4 hours a day, 3 days a week, and earns $12 per hour. How much does she earn in a month (4 weeks)?assistant\n",
      "\n",
      "To find out how much Janet earns in a month, I need to calculate her total earnings for each week and then multiply by 4 weeks. Let's break it down step by step.\n",
      "\n",
      "First, let's figure out how many hours she works in a week. She works 4 hours a day, 3 days a week. So, per week, that's 4 hours/day * 3 days/week. Let me compute that: 4 * 3 = 12 hours per week. So, each week, she works 12 hours.\n",
      "\n",
      "Next, her hourly wage is $12. So, for each hour she works, she earns $12. Therefore, per week, her earnings would be 12 hours/week * $12/hour. Let me calculate that: 12 * 12 = 144 dollars per week. So, in a week, she earns $144.\n",
      "\n",
      "Now, to find out her monthly earnings, I need to multiply her weekly earnings by the number of weeks in a month. A month has 4 weeks. So, 144 * 4. Let's do that: 144 * 4 = 576. So, Janet earns $576 per month.\n",
      "\n",
      "Wait a second, let me double-check my calculations to make sure I didn't make a mistake. \n",
      "\n",
      "First, hours per week: 4 hours/day * 3 days/week. 4*3 is indeed 12. That seems right. Then, per week, 12*12 is 144. That's correct. Then, 144*4 is 576. Hmm, seems straightforward. \n",
      "\n",
      "Alternatively, maybe I can think of it as 4 hours/day * 3 days/week * 4 weeks/month. Let's see: 4*3*4. 4*3 is 12, 12*4 is 48. So, 48 hours/month. Then, 48*12 is 576. Yep, same result. \n",
      "\n",
      "Alternatively, maybe I can use a different approach. Let's see, if she works 4 hours/day, 3 days a week, then per week, that's 4*3=12 hours. Then, per month, that's 12*4=48 hours. Then, 48*12=576. Same answer. \n",
      "\n",
      "I think that's solid. So, the answer should be $576 per month. \n",
      "\n",
      "Just to be thorough, let me check if there's any other factor I might have missed. For example, does the problem specify that she works part-time? The question says \"part-time at a grocery store,\" which I assume means she works a certain number of hours per week, but the exact number isn't given. However, the problem doesn't ask for the number of hours she works per week, just the total hours in a month. So, the calculation is straightforward. \n",
      "\n",
      "Another way to think about it: if she works 4 hours/day, 3 days a week, then in a month, which has 4 weeks, she works 4*3=12 hours/week. Then, 12 hours/week * 4 weeks/month = 48 hours/month. 48 hours/month * $12/hour = $576. Yep, same answer. \n",
      "\n",
      "I don't see any errors in the reasoning. The problem is straightforward, and the answer is consistent with the calculations. So, I think $576 is the correct answer. \n",
      "\n",
      "**Final Answer**\n",
      "\\boxed{576}\n",
      "\n",
      "assistant\n",
      "\n",
      "assistant\n",
      "\n",
      "<|begin_of_thought|>\n",
      "\n",
      "Okay, let's see. Janet works part-time at a grocery store. She works 4 hours a day, 3 days a week, and earns $12 per hour. I need to find out how much she earns in a month, which is 4 weeks. Hmm, part-time means she doesn't work full-time, right? So, maybe the problem is assuming she works a certain number of hours per week, but the exact number isn't given. But the question doesn't ask for the number, just the total in a month. So, the calculation should be straightforward.\n",
      "\n",
      "First, let's figure out how many hours she works in a week. She works 4 hours/day and 3 days a week. So, 4 hours/day * 3 days/week. That's 12 hours per week. Got that. Then, per week, she earns 12 hours * $12/hour. Let me calculate that: 12 * 12 = 144. So, each week, she earns $144.\n",
      "\n",
      "Now, for a month, which has 4 weeks. So, multiply her weekly earnings by 4 weeks. 144 * 4. Let me do that. 144 * 4 is 576. So, $576 per month. That seems right.\n",
      "\n",
      "Wait a second, let me check if there's any other way to approach this. Maybe if she worked more hours per week, but the problem doesn't specify. Since it just says \"part-time,\" I think the number of hours per week isn't required. The calculation is based on the given information. So, the answer should be $576.\n",
      "\n",
      "I don't see any mistakes in the reasoning. The key steps are calculating the hours per week, then multiplying by the number of weeks in the month. All steps are correct. So, I think the final answer is $576.\n",
      "\n",
      "<|end_of_thought|>\n",
      "\n",
      "<|begin_of_solution|>\n",
      "\n",
      "To determine Janet's monthly earnings, we calculate her earnings per week and then multiply by the number of weeks in a month.\n",
      "\n",
      "1. **Hours per Week**:  \n",
      "   She works 4 hours/day and 3 days/week.  \n",
      "   \\( 4 \\, \\text{hours/day} \\times 3 \\, \\text{days/week} = 12 \\, \\text{hours/week} \\).\n",
      "\n",
      "2. **Weekly Earnings**:  \n",
      "   \\( 12 \\, \\text{hours/week} \\times \\$12/\\text{hour} = \\$144 \\, \\text{per week} \\).\n",
      "\n",
      "3. **Monthly Earnings**:  \n",
      "   \\( \\$144 \\, \\text{per week} \\times 4 \\, \\text{weeks/month} = \\$576 \\, \\text{per month} \\).\n",
      "\n",
      "**Answer**: Janet earns \\(\\boxed{576}\\) dollars in a month.\n",
      "\n",
      "<|end_of_solution|>\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load model and tokenizer\"\"\"\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=checkpoint,  # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, prompt):\n",
    "    \"\"\"Generate response for a given prompt\"\"\"\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=2048,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    end_time = time.time()\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generation_time = end_time - start_time\n",
    "    return response, generation_time\n",
    "\n",
    "\n",
    "def compare_models(base_path, finetuned_path, test_prompts):\n",
    "    \"\"\"Compare responses from base and finetuned models\"\"\"\n",
    "    print(\"Loading models...\")\n",
    "    base_model, base_tokenizer = load_model(base_path)\n",
    "    ft_model, ft_tokenizer = load_model(finetuned_path)\n",
    "\n",
    "    FastLanguageModel.for_inference(base_model)\n",
    "    FastLanguageModel.for_inference(ft_model)\n",
    "\n",
    "    results = []\n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"\\nTest {i}/{len(test_prompts)}\")\n",
    "        print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "        # Generate responses\n",
    "        base_response, base_time = generate_response(base_model, base_tokenizer, prompt)\n",
    "        ft_response, ft_time = generate_response(ft_model, ft_tokenizer, prompt)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"base_response\": base_response,\n",
    "                \"base_time\": base_time,\n",
    "                \"ft_response\": ft_response,\n",
    "                \"ft_time\": ft_time,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"Base Model Response ({base_time:.2f}s):\\n{base_response}\\n\")\n",
    "        print(f\"Finetuned Model Response ({ft_time:.2f}s):\\n{ft_response}\\n\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "test_prompts = [\n",
    "    \"Janet works part-time at a grocery store. She works 4 hours a day, 3 days a week, and earns $12 per hour. How much does she earn in a month (4 weeks)?\",\n",
    "]\n",
    "\n",
    "base_model_path = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "finetuned_model_path = (\n",
    "    \"chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF/checkpoint-17500\"\n",
    ")\n",
    "\n",
    "results = compare_models(base_model_path, finetuned_model_path, test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 338.84 out of 503.51 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 119.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['bf16'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF into bf16 GGUF format.\n",
      "The output location will be /workspace/grpo_demo/chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF/unsloth.BF16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {2048, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "Unsloth: Conversion completed! Output location: /workspace/grpo_demo/chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF/unsloth.BF16.gguf\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c762f5f700f542c1b5c68e3ec1ae2db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c912ad06ad78481eba2869176771eb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.BF16.gguf:   0%|          | 0.00/7.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model.push_to_hub_gguf(\n",
    "    \"chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF\",  # Change hf to your username!\n",
    "    tokenizer,\n",
    "    quantization_method=[\"not_quantized\"],\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.5: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.448 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: vLLM loading outputs/2025-02-17/23-27-47/Llama-3.2-1B-Instruct-OpenThought-SFT-GRPO-GGUF with actual GPU utilization = 24.25%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.6 with VRAM = 44.45 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 224.\n",
      "Unsloth: vLLM's KV Cache can use up to 8.39 GB. Also swap space = 6 GB.\n",
      "INFO 02-19 00:15:30 config.py:542] This model supports multiple tasks: {'generate', 'classify', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 02-19 00:15:30 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='outputs/2025-02-17/23-27-47/Llama-3.2-1B-Instruct-OpenThought-SFT-GRPO-GGUF', speculative_config=None, tokenizer='outputs/2025-02-17/23-27-47/Llama-3.2-1B-Instruct-OpenThought-SFT-GRPO-GGUF', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=outputs/2025-02-17/23-27-47/Llama-3.2-1B-Instruct-OpenThought-SFT-GRPO-GGUF, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":224}, use_cached_outputs=False, \n",
      "INFO 02-19 00:15:31 model_runner.py:1110] Starting to load model outputs/2025-02-17/23-27-47/Llama-3.2-1B-Instruct-OpenThought-SFT-GRPO-GGUF...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681fff0dcebe455ea759bbeb13b804b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 00:15:39 model_runner.py:1115] Loading model weights took 2.3048 GB\n",
      "INFO 02-19 00:15:40 worker.py:267] Memory profiling takes 0.62 seconds\n",
      "INFO 02-19 00:15:40 worker.py:267] the current vLLM instance can use total_gpu_memory (44.45GiB) x gpu_memory_utilization (0.24) = 10.78GiB\n",
      "INFO 02-19 00:15:40 worker.py:267] model weights take 2.30GiB; non_torch_memory takes 0.00GiB; PyTorch activation peak memory takes 1.03GiB; the rest of the memory reserved for KV Cache is 7.44GiB.\n",
      "INFO 02-19 00:15:41 executor_base.py:110] # CUDA blocks: 15237, # CPU blocks: 12288\n",
      "INFO 02-19 00:15:41 executor_base.py:115] Maximum concurrency for 2048 tokens per request: 119.04x\n",
      "INFO 02-19 00:15:45 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31/31 [00:28<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-19 00:16:13 model_runner.py:1562] Graph capturing finished in 28 secs, took 0.20 GiB\n",
      "INFO 02-19 00:16:13 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 34.22 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048  # Can increase for longer reasoning traces\n",
    "lora_rank = 64  # Larger rank = smarter, but slower\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"outputs/2025-02-17/23-27-47/Llama-3.2-1B-Instruct-OpenThought-SFT-GRPO-GGUF\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    load_in_4bit=True,  # False for LoRA 16bit\n",
    "    fast_inference=True,  # Enable vLLM fast inference\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.4,  # Reduce if out of memory\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # Remove QKVO if out of memory\n",
    "    lora_alpha=lora_rank,\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Enable long context finetuning\n",
    "    random_state=3407,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:07<00:00,  7.55s/it, est. speed input: 10.34 toks/s, output: 151.79 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_thought|>\n",
      "\n",
      "Okay, so I need to calculate pi, right? Pi is the ratio of a circle's circumference to its diameter. Let me remember how to do that. Hmm. I think the formula is pi equals the circumference divided by the diameter. So, circumference is around a circle, which is 2Ï€r, and diameter is just 2r. So pi is 2Ï€ divided by 2r. That would be Ï€ = 2Ï€rÂ². Wait, let me check that again. If the radius is r, then diameter is 2r. So circumference is 2Ï€r, right? So Ï€ = 2Ï€rÂ². \n",
      "\n",
      "But maybe I should make sure I understand the units. Circumference is in units of length, and diameter is in units of length as well. Since pi is a ratio, it's usually expressed in terms of mathematical terms, not just numbers. So Ï€ should be calculated as the ratio of circumference to diameter. Let me verify with a simple example. If the diameter is 5 cm, then the radius is 2.5 cm. So the circumference is 2Ï€ * 2.5 = 5Ï€ cm. Then dividing by the diameter, which is 5 cm, gives 5Ï€/5 = Ï€ cm. That works. Another example: diameter is 10 cm. Circumference is 2Ï€ * 10 = 20Ï€ cm. So radius is 10 cm, circumference is 20Ï€ cm, diameter is 20 cm, so Ï€ = 20Ï€/20 = Ï€. Yep, that's correct.\n",
      "\n",
      "Wait, but how does this relate to actual numbers? Let me think. For real circles, the circumference of a circle is 2Ï€r, so if you can compute 2Ï€r, that would be the value of pi. So if I can compute r first, then multiply by 2 and divide by 2Ï€, that should give pi. So I need to compute the radius. Let's see, 2Ï€r. If I know the length of a side of a square, say a side length is 1 meter. Then radius is half the side length, so r = 0.5 meters. Then 2Ï€r would be 2 * 3.14 * 0.5, which is 3.14 meters. Then 3.14 meters divided by 3.14 meters equals 1, which is pi. That makes sense. So for real numbers, pi is approximately 3.14159. But since pi is a mathematical constant, it's an irrational number, so it can't be expressed exactly as a decimal. It's approximately 3.1415926535... So how do I compute this? Well, pi is a mathematical constant, so I can compute it numerically using an approximation.\n",
      "\n",
      "But wait, let me double-check my steps. The formula is Ï€ = 2Ï€rÂ². If r is 0.5, then 2Ï€*(0.5)^2. Let's compute that: 2Ï€*(0.25) = 0.5Ï€. Then Ï€ is approximately 3.14159. So yes, that matches. Another way to think about it is that the circumference of a circle is 2Ï€r, and diameter is 2r. So Ï€ = circumference / diameter = (2Ï€r)/2r = Ï€. So that's another way to express it. So in terms of r, Ï€ is equal to 2Ï€r. So depending on the value of r, pi can be calculated, but since we're dealing with a real circle, we have to use real numbers. So if I use the side length as the side of a square, then radius is half the side length, which is 0.5 meters. Then 2Ï€r is 2 * 3.14 * 0.5 = 3.14 meters. 3.14 divided by 3.14 equals 1. So pi is 1. So that's correct.\n",
      "\n",
      "Alternatively, using the formula for the circumference of a circle, which is 2Ï€r. So if r is 0.5, then 2Ï€*(0.5)^2 = 2Ï€*(0.25) = 0.5Ï€. Then Ï€ is approximately 3.1415926535... So even though it's an irrational number, there's a numerical approximation. So how do I get this approximation? Well, the approximation is commonly known as the regular approximation of pi, often denoted as 3.1415926535... So the next term is a decimal after the decimal that changes as we go to more decimal places. For example, after the decimal is 7.5708... So pi is 3.1415926535... So the numerical value is between 3 and 3.14159.\n",
      "\n",
      "I think that's about it. Let me check if there's another way. The definition of pi is 3.1415926535... So all approximations are based on this. So for example, if I take pi as 3.1415926535..., then I can compute it step by step. But since the problem just asks for the value, it's sufficient to say that pi is approximately 3.1415926535... So the final answer is Ï€ â‰ˆ 3.1415926535...\n",
      "\n",
      "<|end_of_thought|>\n",
      "\n",
      "<|begin_of_solution<|||\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "Respond in the following format:\n",
    "<|begin_of_thought|>\n",
    "...\n",
    "<|end_of_thought|>\n",
    "<|begin_of_solution|>\n",
    "...\n",
    "<|end_of_solution|>\n",
    "  \"\"\"\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"Calculate pi.\"},\n",
    "    ],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "from vllm import SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=8192,\n",
    ")\n",
    "output = (\n",
    "    model.fast_generate(\n",
    "        [text],\n",
    "        sampling_params=sampling_params,\n",
    "        lora_request=None,\n",
    "    )[0]\n",
    "    .outputs[0]\n",
    "    .text\n",
    ")\n",
    "\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.34s/it, est. speed input: 25.77 toks/s, output: 147.13 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_thought|>\n",
      "\n",
      "Okay, let's see. I need to figure out what 5 plus 3 minus 1 is. Hmm, let me break this down step by step. So, the problem is 5 plus 3 minus 1. Let me write it out: 5 + 3 - 1. \n",
      "\n",
      "First, I should probably add 5 and 3 first, then subtract 1. Let me add them first. 5 plus 3. That's straightforward, right? 5 plus 3 equals 8. So now the equation is 8 - 1. Then I need to subtract 1 from 8. Subtracting 1 from 8. Hmm, how do I do that? Let me think. 8 minus 1 is 7, so the answer should be 7. \n",
      "\n",
      "Wait, let me double-check to make sure I didn't make a mistake. If I add 5 and 3 first, that's 8. Then subtract 1 from 8, that's 7. Yeah, that seems right. I don't see any other way to approach this. Maybe breaking it down into smaller parts? Like, 5 + 3 would be 8, then 8 - 1 is 7. Yeah, that still gives the same result. \n",
      "\n",
      "I guess another way to think about it is to use order of operations, like PEMDAS. P is parentheses, E is exponents, M is multiplication and division, and D is addition and subtraction. Since there are no parentheses or exponents here, the order is normal addition and subtraction. So the order is right side first, which is 5 + 3, then minus 1. That makes sense. So 5 + 3 is definitely 8, then 8 minus 1 is 7. \n",
      "\n",
      "Wait, let me check using another order. If I did 8 - 1 first, that would be 7. Yeah, same answer. Okay, so both methods work. Maybe I can simplify it by breaking down the numbers. 5 + 3 can be thought of as 8 - 1, which is exactly what I did. That's a good way to check, so I can be confident the answer is 7. \n",
      "\n",
      "<|end_of_thought|>\n",
      "\n",
      "<|end_of_solution|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": \"What is 5 + 3 - 1?\"},\n",
    "    ],\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "from vllm import SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=8192,\n",
    ")\n",
    "output = (\n",
    "    model.fast_generate(\n",
    "        text,\n",
    "        sampling_params=sampling_params,\n",
    "        lora_request=model.load_lora(\n",
    "            \"chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GRPO-GGUF\"\n",
    "        ),\n",
    "    )[0]\n",
    "    .outputs[0]\n",
    "    .text\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file size: 7.47 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base_model_path = \"/workspace/grpo_demo/chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GRPO-GGUF/unsloth.BF16.gguf\"\n",
    "\n",
    "if not os.path.exists(base_model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found: {base_model_path}\")\n",
    "\n",
    "file_size = os.path.getsize(base_model_path)\n",
    "print(f\"Model file size: {file_size / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: You are pushing to hub, but you passed your HF username = chriswhpang.\n",
      "We shall truncate chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GRPO-16bit to Llama-3.2-1B-Instruct-OpenThought-SFT-GRPO-16bit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 270.14 out of 503.51 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 126.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba8afc7e4be477d94be2182401e341c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56d5541c7d54966ab77cbfae1ce257a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaceea95ebeb4c398e66dedc1eeb4a3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64817f42dcc24e2bb76689e499891f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Saved merged model to https://huggingface.co/chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GRPO-16bit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model.push_to_hub_merged(\n",
    "    \"chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GRPO-16bit\",\n",
    "    tokenizer,\n",
    "    save_method=\"merged_16bit\",\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
