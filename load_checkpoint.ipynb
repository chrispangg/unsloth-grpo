{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this script to load from checkpoint and save the checkpoint to huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/grpo_demo/.venv/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 02-17 21:39:56 __init__.py:190] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.2.5: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.448 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.5 patched 16 layers with 16 QKV layers, 16 O layers and 16 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = (\n",
    "    None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    ")\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "checkpoint = \"chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF/checkpoint-17500\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=checkpoint,  # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,  # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",  # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"If a box of apples costs $15 and contains 30 apples, what is the cost of each apple? Also, if I want to buy 7 apples, how much would I need to pay?\",\n",
    "    },\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,  # Must add for generation\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(\n",
    "    input_ids=inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=2048,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "==((====))==  Unsloth 2025.2.5: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.448 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.5: Fast Llama patching. Transformers: 4.48.3.\n",
      "   \\\\   /|    GPU: NVIDIA A40. Max memory: 44.448 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "Test 1/1\n",
      "Prompt: Janet works part-time at a grocery store. She works 4 hours a day, 3 days a week, and earns $12 per hour. How much does she earn in a month (4 weeks)?\n",
      "\n",
      "Base Model Response (20.52s):\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "user\n",
      "\n",
      "Janet works part-time at a grocery store. She works 4 hours a day, 3 days a week, and earns $12 per hour. How much does she earn in a month (4 weeks)?assistant\n",
      "\n",
      "To find out how much Janet earns in a month, I need to calculate her total earnings for 4 weeks. Let's break down the problem step by step.\n",
      "\n",
      "First, let's figure out how many hours she works in a month. Since she works 4 hours a day, 3 days a week, and there are 4 weeks in a month, I can multiply the number of hours per day by the number of days per week and then by the number of weeks.\n",
      "\n",
      "Calculating hours per week: 4 hours/day * 3 days/week = 12 hours/week.\n",
      "\n",
      "Then, multiply that by the number of weeks in the month: 12 hours/week * 4 weeks/month = 48 hours/month.\n",
      "\n",
      "Now, Janet's hourly wage is $12. So, her monthly earnings would be total hours multiplied by hourly wage.\n",
      "\n",
      "Total earnings = 48 hours/month * $12/hour = $576/month.\n",
      "\n",
      "Wait, let me double-check that. If she works 4 hours a day, 3 days a week, that's 4*3=12 hours per week. Then 12 hours/week *4 weeks/month=48 hours/month. Yes, that seems right. So 48 hours times $12 per hour is $576. That checks out.\n",
      "\n",
      "Alternatively, another way to calculate is to convert the hours to days and then multiply by the weekly wage. But since the problem gives hours per day, that's straightforward. So 4 hours/day * 3 days/week * 4 weeks/month = 48 hours/month. Then 48 *12 = 576. Yep, same result.\n",
      "\n",
      "I think that's correct. Let me just make sure there's no trick in the question. It says she works 4 hours a day, 3 days a week. So each day she works 4 hours, each week 3 days, and each month 4 weeks. So the calculation is straightforward. No hidden assumptions here. The answer is $576 per month.\n",
      "\n",
      "**Final Answer**\n",
      "\\boxed{576}\n",
      "\n",
      "assistant\n",
      "\n",
      "assistant\n",
      "\n",
      "<|begin_of_thought|>\n",
      "\n",
      "Okay, let's see. Janet works part-time at a grocery store. She works 4 hours a day, 3 days a week, and earns $12 per hour. I need to find out how much she earns in a month, which is 4 weeks. Hmm, let's break this down step by step.\n",
      "\n",
      "First, I need to figure out how many hours she works in a month. Since she works 4 hours a day and 3 days a week, I should multiply those two numbers together. That makes sense because if you have more hours in a day and more days in a week, you get more hours in a month. So 4 hours/day * 3 days/week = 12 hours/week. Then, multiply that by the number of weeks in a month. There are 4 weeks in a month, so 12 hours/week * 4 weeks/month = 48 hours/month. That seems right.\n",
      "\n",
      "Now, she earns $12 per hour. So, to find her monthly earnings, I multiply the total hours by her hourly wage. That would be 48 hours * $12/hour. Let me do that calculation. 48 * 12. Hmm, 40 * 12 is 480, and 8 * 12 is 96, so 480 + 96 = 576. So that's $576 per month.\n",
      "\n",
      "Wait, let me check again to make sure I didn't make a mistake. If she works 4 hours a day, 3 days a week, that's 4*3=12 hours per week. Then 12 hours/week *4 weeks/month=48 hours/month. Yes, that's correct. So 48 hours *12 is indeed 576. I think that's right.\n",
      "\n",
      "Alternatively, another way to think about it is to convert the hours to days and then multiply by the weekly wage. So 4 hours/day *3 days/week *4 weeks/month=48 hours/month. Then 48*12=576. Same answer. So that's another way to confirm it.\n",
      "\n",
      "I don't see any mistakes in the reasoning here. The key steps are calculating the total hours per month, then multiplying by the hourly wage. All the numbers check out. So the final answer should be $576 per month.\n",
      "\n",
      "<|end_of_thought|>\n",
      "\n",
      "<|begin_of_solution|>\n",
      "\n",
      "To determine Janet's monthly earnings:\n",
      "\n",
      "1. **Calculate total hours per month**:  \n",
      "   - Hours per day: \\(4 \\, \\text{hours/day}\\)  \n",
      "   - Hours per week: \\(4 \\, \\text{hours/day} \\times 3 \\, \\text{days/week} = 12 \\, \\text{hours/week}\\)  \n",
      "   - Hours per month: \\(12 \\, \\text{hours/week} \\times 4 \\, \\text{weeks/month} = 48 \\, \\text{hours/month}\\).\n",
      "\n",
      "2. **Multiply by hourly wage**:  \n",
      "   - Hourly wage: $12 \\, \\text{per hour}  \n",
      "   - Total earnings: \\(48 \\, \\text{hours/month} \\times 12 \\, \\text{per hour} = 576 \\, \\text{dollars/month}\\).\n",
      "\n",
      "**Answer**: Janet earns \\(\\boxed{576}\\) dollars per month.\n",
      "\n",
      "<|end_of_solution|>\n",
      "\n",
      "Finetuned Model Response (24.04s):\n",
      "system\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 July 2024\n",
      "\n",
      "user\n",
      "\n",
      "Janet works part-time at a grocery store. She works 4 hours a day, 3 days a week, and earns $12 per hour. How much does she earn in a month (4 weeks)?assistant\n",
      "\n",
      "To find out how much Janet earns in a month, I need to calculate her total earnings for each week and then multiply by 4 weeks. Let's break it down step by step.\n",
      "\n",
      "First, let's figure out how many hours she works in a week. She works 4 hours a day, 3 days a week. So, per week, that's 4 hours/day * 3 days/week. Let me compute that: 4 * 3 = 12 hours per week. So, each week, she works 12 hours.\n",
      "\n",
      "Next, her hourly wage is $12. So, for each hour she works, she earns $12. Therefore, per week, her earnings would be 12 hours/week * $12/hour. Let me calculate that: 12 * 12 = 144 dollars per week. So, in a week, she earns $144.\n",
      "\n",
      "Now, to find out her monthly earnings, I need to multiply her weekly earnings by the number of weeks in a month. A month has 4 weeks. So, 144 * 4. Let's do that: 144 * 4 = 576. So, Janet earns $576 per month.\n",
      "\n",
      "Wait a second, let me double-check my calculations to make sure I didn't make a mistake. \n",
      "\n",
      "First, hours per week: 4 hours/day * 3 days/week. 4*3 is indeed 12. That seems right. Then, per week, 12*12 is 144. That's correct. Then, 144*4 is 576. Hmm, seems straightforward. \n",
      "\n",
      "Alternatively, maybe I can think of it as 4 hours/day * 3 days/week * 4 weeks/month. Let's see: 4*3*4. 4*3 is 12, 12*4 is 48. So, 48 hours/month. Then, 48*12 is 576. Yep, same result. \n",
      "\n",
      "Alternatively, maybe I can use a different approach. Let's see, if she works 4 hours/day, 3 days a week, then per week, that's 4*3=12 hours. Then, per month, that's 12*4=48 hours. Then, 48*12=576. Same answer. \n",
      "\n",
      "I think that's solid. So, the answer should be $576 per month. \n",
      "\n",
      "Just to be thorough, let me check if there's any other factor I might have missed. For example, does the problem specify that she works part-time? The question says \"part-time at a grocery store,\" which I assume means she works a certain number of hours per week, but the exact number isn't given. However, the problem doesn't ask for the number of hours she works per week, just the total hours in a month. So, the calculation is straightforward. \n",
      "\n",
      "Another way to think about it: if she works 4 hours/day, 3 days a week, then in a month, which has 4 weeks, she works 4*3=12 hours/week. Then, 12 hours/week * 4 weeks/month = 48 hours/month. 48 hours/month * $12/hour = $576. Yep, same answer. \n",
      "\n",
      "I don't see any errors in the reasoning. The problem is straightforward, and the answer is consistent with the calculations. So, I think $576 is the correct answer. \n",
      "\n",
      "**Final Answer**\n",
      "\\boxed{576}\n",
      "\n",
      "assistant\n",
      "\n",
      "assistant\n",
      "\n",
      "<|begin_of_thought|>\n",
      "\n",
      "Okay, let's see. Janet works part-time at a grocery store. She works 4 hours a day, 3 days a week, and earns $12 per hour. I need to find out how much she earns in a month, which is 4 weeks. Hmm, part-time means she doesn't work full-time, right? So, maybe the problem is assuming she works a certain number of hours per week, but the exact number isn't given. But the question doesn't ask for the number, just the total in a month. So, the calculation should be straightforward.\n",
      "\n",
      "First, let's figure out how many hours she works in a week. She works 4 hours/day and 3 days a week. So, 4 hours/day * 3 days/week. That's 12 hours per week. Got that. Then, per week, she earns 12 hours * $12/hour. Let me calculate that: 12 * 12 = 144. So, each week, she earns $144.\n",
      "\n",
      "Now, for a month, which has 4 weeks. So, multiply her weekly earnings by 4 weeks. 144 * 4. Let me do that. 144 * 4 is 576. So, $576 per month. That seems right.\n",
      "\n",
      "Wait a second, let me check if there's any other way to approach this. Maybe if she worked more hours per week, but the problem doesn't specify. Since it just says \"part-time,\" I think the number of hours per week isn't required. The calculation is based on the given information. So, the answer should be $576.\n",
      "\n",
      "I don't see any mistakes in the reasoning. The key steps are calculating the hours per week, then multiplying by the number of weeks in the month. All steps are correct. So, I think the final answer is $576.\n",
      "\n",
      "<|end_of_thought|>\n",
      "\n",
      "<|begin_of_solution|>\n",
      "\n",
      "To determine Janet's monthly earnings, we calculate her earnings per week and then multiply by the number of weeks in a month.\n",
      "\n",
      "1. **Hours per Week**:  \n",
      "   She works 4 hours/day and 3 days/week.  \n",
      "   \\( 4 \\, \\text{hours/day} \\times 3 \\, \\text{days/week} = 12 \\, \\text{hours/week} \\).\n",
      "\n",
      "2. **Weekly Earnings**:  \n",
      "   \\( 12 \\, \\text{hours/week} \\times \\$12/\\text{hour} = \\$144 \\, \\text{per week} \\).\n",
      "\n",
      "3. **Monthly Earnings**:  \n",
      "   \\( \\$144 \\, \\text{per week} \\times 4 \\, \\text{weeks/month} = \\$576 \\, \\text{per month} \\).\n",
      "\n",
      "**Answer**: Janet earns \\(\\boxed{576}\\) dollars in a month.\n",
      "\n",
      "<|end_of_solution|>\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load model and tokenizer\"\"\"\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=checkpoint,  # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, prompt):\n",
    "    \"\"\"Generate response for a given prompt\"\"\"\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs,\n",
    "            max_new_tokens=2048,\n",
    "            temperature=0.1,\n",
    "            do_sample=True,\n",
    "            use_cache=True,\n",
    "        )\n",
    "    end_time = time.time()\n",
    "\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generation_time = end_time - start_time\n",
    "    return response, generation_time\n",
    "\n",
    "\n",
    "def compare_models(base_path, finetuned_path, test_prompts):\n",
    "    \"\"\"Compare responses from base and finetuned models\"\"\"\n",
    "    print(\"Loading models...\")\n",
    "    base_model, base_tokenizer = load_model(base_path)\n",
    "    ft_model, ft_tokenizer = load_model(finetuned_path)\n",
    "\n",
    "    FastLanguageModel.for_inference(base_model)\n",
    "    FastLanguageModel.for_inference(ft_model)\n",
    "\n",
    "    results = []\n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        print(f\"\\nTest {i}/{len(test_prompts)}\")\n",
    "        print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "        # Generate responses\n",
    "        base_response, base_time = generate_response(base_model, base_tokenizer, prompt)\n",
    "        ft_response, ft_time = generate_response(ft_model, ft_tokenizer, prompt)\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"prompt\": prompt,\n",
    "                \"base_response\": base_response,\n",
    "                \"base_time\": base_time,\n",
    "                \"ft_response\": ft_response,\n",
    "                \"ft_time\": ft_time,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(f\"Base Model Response ({base_time:.2f}s):\\n{base_response}\\n\")\n",
    "        print(f\"Finetuned Model Response ({ft_time:.2f}s):\\n{ft_response}\\n\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "test_prompts = [\n",
    "    \"Janet works part-time at a grocery store. She works 4 hours a day, 3 days a week, and earns $12 per hour. How much does she earn in a month (4 weeks)?\",\n",
    "]\n",
    "\n",
    "base_model_path = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "finetuned_model_path = (\n",
    "    \"chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF/checkpoint-17500\"\n",
    ")\n",
    "\n",
    "results = compare_models(base_model_path, finetuned_model_path, test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 338.84 out of 503.51 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 119.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n",
      "Done.\n",
      "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
      "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
      "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
      "\\        /    [2] Converting GGUF 16bits to ['bf16'] might take 10 minutes each.\n",
      " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
      "\n",
      "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
      "Unsloth: [1] Converting model at chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF into bf16 GGUF format.\n",
      "The output location will be /workspace/grpo_demo/chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF/unsloth.BF16.gguf\n",
      "This might take 3 minutes...\n",
      "INFO:hf-to-gguf:Loading model: Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.bfloat16 --> BF16, shape = {2048, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.bfloat16 --> BF16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.bfloat16 --> BF16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.bfloat16 --> BF16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.bfloat16 --> BF16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.bfloat16 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 32\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "Unsloth: Conversion completed! Output location: /workspace/grpo_demo/chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF/unsloth.BF16.gguf\n",
      "Unsloth: Uploading GGUF to Huggingface Hub...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c762f5f700f542c1b5c68e3ec1ae2db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c912ad06ad78481eba2869176771eb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsloth.BF16.gguf:   0%|          | 0.00/7.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GGUF to https://huggingface.co/chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model.push_to_hub_gguf(\n",
    "    \"chriswhpang/Llama-3.2-1B-Instruct-OpenThought-SFT-GGUF\",  # Change hf to your username!\n",
    "    tokenizer,\n",
    "    quantization_method=[\"not_quantized\"],\n",
    "    token=os.getenv(\"HF_TOKEN\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
